{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (408397569.py, line 155)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 155\u001b[0;36m\u001b[0m\n\u001b[0;31m    subbands =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torchaudio as ta\n",
    "import pytorch_lightning as pl\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import mir_eval\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "# from train import RTBWETrain\n",
    "# from datamodule import *\n",
    "from utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from pesq import pesq\n",
    "from pystoi import stoi\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import soundfile as sf\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SEANet import SEANet\n",
    "from NewSEANet import NewSEANet\n",
    "from MelGAN import Discriminator_MelGAN\n",
    "from MBSTFTD import MultiBandSTFTDiscriminator\n",
    "\n",
    "from dataset import PQMFDataset, collate_fn_gt\n",
    "# from ssdiscriminatorblock import MultiBandSTFTDiscriminator\n",
    "\n",
    "DEVICE = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## Dictionary to store all models and information\n",
    "TPARAMS = {}\n",
    "\n",
    "NOTES = 'baseline_pqmf'\n",
    "START_DATE = NOTES +'_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def wandb_log(loglist, epoch, note):\n",
    "    for key, val in loglist.items():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            item = val.cpu().detach().numpy()\n",
    "            \n",
    "        else:\n",
    "            item = val\n",
    "\n",
    "        try:\n",
    "            if isinstance(item, float):\n",
    "                log = item\n",
    "            elif isinstance(item, plt.Figure):\n",
    "                log = wandb.Image(item)\n",
    "                plt.close(item)\n",
    "            elif item.ndim in [2, 3]:  # 이미지 데이터\n",
    "                log = wandb.Image(item, caption=f\"{note.capitalize()} {key.capitalize()} Epoch {epoch}\")\n",
    "            elif item.ndim == 1:  # 오디오 데이터\n",
    "                log = wandb.Audio(item, sample_rate=16000, caption=f\"{note.capitalize()} {key.capitalize()} Epoch {epoch}\")\n",
    "            else:\n",
    "                log = item\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to log {key}: {e}\")\n",
    "            log = item\n",
    "\n",
    "        wandb.log({\n",
    "            f\"{note.capitalize()} {key.capitalize()}\": log,\n",
    "        }, step=epoch)\n",
    "\n",
    "#########################\n",
    "def train_step(train_parameters):\n",
    "    train_parameters['generator'].train()\n",
    "    train_parameters['discriminator'].train()\n",
    "    result = {}\n",
    "    result['loss_G'] = 0\n",
    "    result['loss_D'] = 0\n",
    "    result['FM_loss'] = 0  \n",
    "    result['Mel_loss'] = 0  \n",
    "\n",
    "    train_bar = tqdm(train_parameters['train_dataloader'], desc=\"Train\", position=1, leave=False, disable=False)\n",
    "    i = 0\n",
    "\n",
    "    # Train DataLoader Loop\n",
    "    # Epoch 1개만큼 train\n",
    "    for lf, hf, gt, _ in train_bar:\n",
    "        i += 1\n",
    "        lf = lf.to(DEVICE)\n",
    "        hf = hf.to(DEVICE)\n",
    "        gt = gt.to(DEVICE)\n",
    "        high_extend = train_parameters['generator'](lf, gt)\n",
    "        \n",
    "        '''\n",
    "        Train Generator\n",
    "        '''        \n",
    "        train_parameters['optim_G'].zero_grad()\n",
    "        _, loss_GAN, loss_FM, loss_mel = train_parameters['discriminator'].loss_G(high_extend, hf)\n",
    "        ## MS Mel Loss\n",
    "        loss_G = loss_GAN + 100*loss_FM + loss_mel\n",
    "        loss_G.mean().backward()\n",
    "        train_parameters['optim_G'].step()\n",
    "\n",
    "        '''\n",
    "        Discriminator\n",
    "        '''\n",
    "        train_parameters['optim_D'].zero_grad()\n",
    "        loss_D = train_parameters['discriminator'].loss_D(high_extend, hf)\n",
    "        loss_D.mean().backward()\n",
    "        train_parameters['optim_D'].step()\n",
    "        \n",
    "        result['loss_G'] += loss_G\n",
    "        result['loss_D'] += loss_D\n",
    "        result['FM_loss'] += loss_FM  # FM loss 추가\n",
    "        result['Mel_loss'] += loss_mel  # Mel loss 추가\n",
    "\n",
    "        train_bar.set_postfix({\n",
    "                'Loss G': f'{loss_G.item():.2f}',\n",
    "                'FM loss': f'{loss_FM.item():.2f}',\n",
    "                'Mel loss': f'{loss_mel.item():.2f}',\n",
    "                'Loss D': f'{loss_D.item():.2f}'\n",
    "            })\n",
    "        \n",
    "        del lf, hf, loss_GAN, loss_FM, loss_mel, loss_D\n",
    "        gc.collect()\n",
    "\n",
    "    train_bar.close()\n",
    "    result['loss_G'] /= len(train_parameters['train_dataloader'])\n",
    "    result['loss_D'] /= len(train_parameters['train_dataloader'])\n",
    "    result['FM_loss'] /= len(train_parameters['train_dataloader'])  # FM loss 평균 계산\n",
    "    result['Mel_loss'] /= len(train_parameters['train_dataloader'])  # Mel loss 평균 계산\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "def test_step(test_parameters, pqmf, store_lr_hr=False):\n",
    "    test_parameters['generator'].eval()\n",
    "    test_parameters['discriminator'].eval()\n",
    "    result = {}\n",
    "    result['loss_G'] = 0\n",
    "    result['loss_D'] = 0\n",
    "    result['FM_loss'] = 0  # FM loss 결과 추가\n",
    "    result['Mel_loss'] = 0  # Mel loss 결과 추가\n",
    "    result['PESQ'] = 0  # PESQ 결과 추가\n",
    "    result['LSD'] = 0\n",
    "\n",
    "    test_bar = tqdm(test_parameters['val_dataloader'], desc='Validation', position=1, leave=False, disable=False)\n",
    "\n",
    "    i = 0\n",
    "    total_pesq = 0\n",
    "    total_lsd = 0\n",
    "    total_lsd_h = 0\n",
    "    # Test DataLoader Loop\n",
    "    with torch.no_grad():\n",
    "        for lf, hf, gt, _ in test_bar:\n",
    "            i += 1\n",
    "            lf = lf.to(DEVICE)\n",
    "            hf = hf.to(DEVICE)\n",
    "            gt = gt.to(DEVICE)\n",
    "            high_extend = test_parameters['generator'](lf, gt)\n",
    "            \n",
    "            _, loss_GAN, loss_FM, loss_mel = test_parameters['discriminator'].loss_G(high_extend, hf)\n",
    "            loss_G = loss_GAN + 100*loss_FM + loss_mel\n",
    "            loss_D = test_parameters['discriminator'].loss_D(high_extend, hf)\n",
    "            \n",
    "            result['loss_G'] += loss_G\n",
    "            result['loss_D'] += loss_D\n",
    "            result['FM_loss'] += loss_FM  # FM loss \n",
    "            result['Mel_loss'] += loss_mel  # Mel loss \n",
    "\n",
    "            ##### Target signal is HF signal\n",
    "            subbands = torch.stack((lf, hf), dim=1)\n",
    "            pqmf.to(DEVICE)\n",
    "            bwe = pqmf.synthesis(subbands)\n",
    "\n",
    "            pesq_score =  pesq(fs=16000, ref=gt.squeeze().cpu().numpy(), deg=bwe.squeeze().cpu().numpy(), mode=\"wb\")\n",
    "            total_pesq += pesq_score\n",
    "            \n",
    "            batch_lsd = lsd_batch(x_batch=gt.cpu(), y_batch=bwe.cpu())\n",
    "            total_lsd += batch_lsd\n",
    "\n",
    "            batch_lsd_h = lsd_batch(x_batch=gt.cpu(), y_batch=bwe.cpu(), start=4000, cutoff_freq=8000)\n",
    "            total_lsd_h += batch_lsd_h\n",
    "\n",
    "            test_bar.set_postfix({\n",
    "                'Loss G': f'{loss_G.item():.2f}',\n",
    "                'FM loss': f'{loss_FM.item():.2f}',\n",
    "                'Mel loss': f'{loss_mel.item():.2f}',\n",
    "                'Loss D': f'{loss_D.item():.2f}',\n",
    "                'PESQ': f'{pesq_score:.2f}',\n",
    "                'LSD': f'{batch_lsd:.2f}' \n",
    "            })\n",
    "\n",
    "            if i == 500 and store_lr_hr:  # For very first epoch\n",
    "                result['audio_GT'] = gt.squeeze().cpu().numpy()\n",
    "                result['spec_lf'] = draw_spec(lf.squeeze().cpu().numpy(),win_len=320//2, hop_len=160//2, return_fig=True)\n",
    "                result['spec_hf'] = draw_spec(hf.squeeze().cpu().numpy(),win_len=320//2, hop_len=160//2, return_fig=True)\n",
    "                result['spec_GT'] = draw_spec(gt.squeeze().cpu().numpy(),win_len=320, hop_len=160, return_fig=True)\n",
    "\n",
    "            if i == 500:\n",
    "                result['audio_bwe'] = bwe.squeeze().cpu().numpy()\n",
    "                result['spec_bwe'] = draw_spec(bwe.squeeze().cpu().numpy(),win_len=320, hop_len=160, return_fig=True)\n",
    "                # result['audio_hf_gen'] = high_extend.squeeze().cpu().numpy()\n",
    "                result['spec_hf_gen'] = draw_spec(high_extend.squeeze().cpu().numpy(),win_len=320//2, hop_len=160//2, return_fig=True)\n",
    "\n",
    "            del lf, hf, bwe, subbands, high_extend, loss_GAN, loss_FM, loss_mel, loss_D\n",
    "            gc.collect()\n",
    "\n",
    "        test_bar.close()\n",
    "        result['loss_G'] /= len(test_parameters['val_dataloader'])\n",
    "        result['loss_D'] /= len(test_parameters['val_dataloader'])\n",
    "        result['FM_loss'] /= len(test_parameters['val_dataloader'])  # FM loss 평균 계산\n",
    "        result['Mel_loss'] /= len(test_parameters['val_dataloader'])  # Mel loss 평균 계산\n",
    "        result['PESQ'] = total_pesq / len(test_parameters['val_dataloader'])\n",
    "        result['LSD'] = total_lsd / len(test_parameters['val_dataloader'])\n",
    "        result['LSD_H'] = total_lsd_h / len(test_parameters['val_dataloader'])\n",
    "        \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    ################ Read Config Files\n",
    "    config = yaml.load(open(\"./config_baseline_pqmf.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "    wandb.init(project='SSLBWE_phase2',\n",
    "           entity='woongzip1',\n",
    "           config=config,\n",
    "           name=START_DATE,\n",
    "           # mode='disabled',\n",
    "           notes=NOTES)\n",
    "\n",
    "    ## Load Dataset\n",
    "    base_path = \"/mnt/hdd/Dataset/VCTK-PQMF/16k\"\n",
    "    train_dataset = PQMFDataset(base_path, seg_len=2, mode='train')\n",
    "    valid_dataset = PQMFDataset(base_path, seg_len=2, mode='test')\n",
    "\n",
    "    # 데이터셋에서 일부분만 추출하여 Subset 데이터셋 생성\n",
    "    num_train_samples = 1000  # 사용할 train 데이터셋의 샘플 수\n",
    "    num_valid_samples = 200   # 사용할 validation 데이터셋의 샘플 수\n",
    "    train_indices = random.sample(range(len(train_dataset)), num_train_samples)\n",
    "    valid_indices = random.sample(range(len(valid_dataset)), num_valid_samples)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    valid_subset = Subset(valid_dataset, valid_indices)\n",
    "\n",
    "    pqmf = PQMF(N=2, taps=62, cutoff=0.25, beta=9.0)\n",
    "    print(f'Train Dataset size: {len(train_dataset)} | Validation Dataset size: {len(valid_dataset)}\\n')\n",
    "    \n",
    "    ################ Load DataLoader\n",
    "    TPARAMS['train_dataloader'] = DataLoader(train_dataset, batch_size = config['dataset']['batch_size'], shuffle=True, \n",
    "                                            collate_fn=lambda batch: collate_fn_gt(batch, pqmf),\n",
    "                                            num_workers=config['dataset']['num_workers'], prefetch_factor=2, persistent_workers=True)\n",
    "    TPARAMS['val_dataloader'] = DataLoader(valid_dataset, batch_size = 1, shuffle=False, \n",
    "                                            collate_fn=lambda batch: collate_fn_gt(batch, pqmf),\n",
    "                                            num_workers=config['dataset']['num_workers'], prefetch_factor=2, persistent_workers=True)  \n",
    "     \n",
    "    print(f\"DataLoader Loaded!: {len(TPARAMS['train_dataloader'])} | {len(TPARAMS['val_dataloader'])}\")\n",
    "\n",
    "    ################ Load Models\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"torch.nn.utils.weight_norm is deprecated\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"`resume_download` is deprecated\")\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR.*\")\n",
    "\n",
    "    gen_type = config['model']['generator']\n",
    "\n",
    "    print(\"########################################\")\n",
    "    if gen_type == \"SEANet\":\n",
    "        TPARAMS['generator'] = SEANet(min_dim=8, causality=True)\n",
    "        print(f\"SEANet Generator: \\n\"\n",
    "    f\"                  Disc: {config['model']['discriminator']}\")\n",
    "    elif gen_type == \"NewSEANet\":\n",
    "        TPARAMS['generator'] = NewSEANet(min_dim=8, \n",
    "                                       kmeans_model_path=config['model']['kmeans_path'],\n",
    "                                       modelname=config['model']['sslname'],\n",
    "                                       causality=True)\n",
    "        print(f\"NewSEANeT + {config['model']['sslname']} Generator: \\n\"\n",
    "    f\"                  kmeans:{os.path.splitext(os.path.basename(config['model']['kmeans_path']))[0]} \\n\"\n",
    "    f\"                  Disc: {config['model']['discriminator']},\\n\"\n",
    "    f\"                  SSL model: {config['model']['sslname']}\")\n",
    "    else: \n",
    "        raise ValueError(f\"Unsupported generator type: {gen_type}\")\n",
    "\n",
    "    disc_type = config['model']['discriminator']\n",
    "    if disc_type == \"MSD\":\n",
    "        TPARAMS['discriminator'] = Discriminator_MelGAN()\n",
    "    elif disc_type == \"MBSTFTD\":\n",
    "        discriminator_config = config['model']['MultiBandSTFTDiscriminator_config']\n",
    "        TPARAMS['discriminator'] = MultiBandSTFTDiscriminator(\n",
    "            C=discriminator_config['C'],\n",
    "            n_fft_list=discriminator_config['n_fft_list'],\n",
    "            hop_len_list=discriminator_config['hop_len_list'],\n",
    "            bands=discriminator_config['band_split_ratio'],\n",
    "            ms_mel_loss_config=config['model']['ms_mel_loss_config']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported discriminator type: {disc_type}\")\n",
    "    print(\"########################################\")\n",
    "    ################ Load Optimizers\n",
    "    TPARAMS['optim_G'] = torch.optim.Adam(TPARAMS['generator'].parameters(), lr=config['optim']['learning_rate'], \n",
    "                                          betas=(config['optim']['B1'],config['optim']['B2']))\n",
    "    TPARAMS['optim_D'] = torch.optim.Adam(TPARAMS['discriminator'].parameters(), config['optim']['learning_rate'], \n",
    "                                          betas=(config['optim']['B1'],config['optim']['B2']))\n",
    "    \n",
    "    ################ Load Checkpoint if available\n",
    "    start_epoch = 1\n",
    "    best_pesq = -1\n",
    "\n",
    "    if config['train']['ckpt']:\n",
    "        checkpoint_path = config['train']['ckpt_path']\n",
    "        if os.path.isfile(checkpoint_path):\n",
    "            start_epoch, best_pesq = load_checkpoint(TPARAMS['generator'], TPARAMS['discriminator'],\n",
    "                                                     TPARAMS['optim_G'], TPARAMS['optim_D'], checkpoint_path)\n",
    "        else:\n",
    "            print(f\"Checkpoint file not found at {checkpoint_path}. Starting training from scratch.\")\n",
    "\n",
    "    ################ Training Code\n",
    "    print('Train Start!!')\n",
    "    BAR = tqdm(range(start_epoch, config['train']['max_epochs'] + 1), position=0, leave=True)\n",
    "    TPARAMS['generator'].to(DEVICE)\n",
    "    TPARAMS['discriminator'].to(DEVICE)\n",
    "    best_pesq = -1\n",
    "\n",
    "    store_lr_hr = True # flag\n",
    "    for epoch in BAR:\n",
    "        # set_seed(epoch+42)\n",
    "\n",
    "        TPARAMS['current_epoch'] = epoch\n",
    "        train_result = train_step(TPARAMS)\n",
    "        wandb_log(train_result, epoch, 'train')\n",
    "\n",
    "        if epoch % config['train']['val_epoch'] == 0:\n",
    "            # Validation step\n",
    "            val_result = test_step(TPARAMS, pqmf, store_lr_hr)\n",
    "            wandb_log(val_result, epoch, 'val')\n",
    "\n",
    "\n",
    "            if store_lr_hr:\n",
    "                store_lr_hr = False\n",
    "\n",
    "            if val_result['PESQ'] > best_pesq:\n",
    "                best_pesq = val_result['PESQ']\n",
    "                save_checkpoint(TPARAMS['generator'], TPARAMS['discriminator'], epoch, best_pesq, val_result['LSD'])\n",
    "\n",
    "            desc = (f\"Epoch [{epoch}/{config['train']['max_epochs']}] \"\n",
    "                    f\"Loss G: {train_result['loss_G']:.2f}, \"\n",
    "                    f\"FM Loss: {train_result['FM_loss']:.2f}, \"\n",
    "                    f\"Mel Loss: {train_result['Mel_loss']:.2f}, \"\n",
    "                    f\"Loss D: {train_result['loss_D']:.2f}, \"\n",
    "                    f\"Val Loss G: {val_result['loss_G']:.2f}, \"\n",
    "                    f\"Val FM Loss: {val_result['FM_loss']:.2f}, \"\n",
    "                    f\"Val Mel Loss: {val_result['Mel_loss']:.2f}, \"\n",
    "                    f\"Val Loss D: {val_result['loss_D']:.2f}, \"\n",
    "                    f\"LSD: {val_result['LSD']:.2f}\"\n",
    "                    )\n",
    "            \n",
    "        else:\n",
    "            desc = (f\"Epoch [{epoch}/{config['train']['max_epochs']}] \"\n",
    "                    f\"Loss G: {train_result['loss_G']:.2f}, \"\n",
    "                    f\"FM Loss: {train_result['FM_loss']:.2f}, \"\n",
    "                    f\"Mel Loss: {train_result['Mel_loss']:.2f}, \"\n",
    "                    f\"Loss D: {train_result['loss_D']:.2f}\"\n",
    "                    )\n",
    "        BAR.set_description(desc)\n",
    "\n",
    "    gc.collect()\n",
    "    final_epoch = config['train']['max_epochs']\n",
    "    save_checkpoint(TPARAMS['generator'], TPARAMS['discriminator'], final_epoch, val_result['PESQ'], val_result['LSD'])\n",
    "\n",
    "def save_checkpoint(generator, discriminator, epoch, pesq_score, lsd, config):\n",
    "    checkpoint_dir = config['train']['ckpt_save_dir']\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_pesq_{pesq_score:.2f}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': TPARAMS['optim_G'].state_dict(),  # Save optimizer state\n",
    "        'optimizer_D_state_dict': TPARAMS['optim_D'].state_dict(),  # Save optimizer state\n",
    "        'pesq_score': pesq_score,\n",
    "        'lsd': lsd,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(generator, discriminator, optimizer_G, optimizer_D, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_pesq = checkpoint['pesq_score']\n",
    "    \n",
    "    if 'optimizer_G_state_dict' in checkpoint and 'optimizer_D_state_dict' in checkpoint:\n",
    "        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "\n",
    "    return start_epoch, best_pesq\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
